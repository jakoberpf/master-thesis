\chapter{Applied Statistic Methods}
\label{definition_correlation}
This chapter will explain the implemented statistical methods and their mathematical definitions. This should result in a deep understanding how the results which are presented in the upcoming chapters are comprised. It also clarifies which methods and assumptions are implemented to handle a mixed data-based analysis.

\bigskip

Correlation is an analysis procedure that measures the correlation coefficient, which represents the degree of linear, bivariant, monotonic or other kind of relation, which could also be described as the degree of association between two variables \parencite{HerzSchlicherSiegener1992}. In most statistics the following common types can be found: Pearson's $r$, Kendall's $\tau$, Spearman  $\rho$ or the Point-Biserial correlation \parencite{Ramzai2020,SPSS2020a,SPSS2020b}. Besides off these, there are many more correlation coefficients which vary in their applicability and interpretability. Depending on which type of data variables are to be analyzed, it is necessary to choose an applicable correlation coefficient. The type of data variable and relation combination are the most restricting features for choosing a suitable correlation coefficient. 

\section{Variable Types}
\label{correlation_variable_types}
Data variables can be grouped into continuous and categorical variables, depending on what kind of observation they describe. Variables are considered to be continuous, also known as quantitative, if they relate to measurements like speed, distance or age, which can take on an unlimited number of values between the lowest and highest points of measurement \parencite{McCue2007}. These continuous variables can be separated into two subsets. 

\begin{itemize}
	\item \textbf{Interval} variables can be measured along a continuum and have a numerical value \parencite{Laerd2020}.
    \item \textbf{Ratio} variables are interval variables, with the added condition that the values are set to zero if there is no measurement for this value \parencite{Laerd2020}.
\end{itemize}

Categorical variables on the other hand are limited in the number of values, referring to a category, rank or choice, like a vehicle type or Yes/No answers. These categorical variables can be separated into three subsets.

\begin{itemize}
	\item \textbf{Nominal} variables have two or more categories, but with no intrinsic order \parencite{Laerd2020}.
	\item \textbf{Dichotomous} are nominal variables which have only two categories or levels \parencite{Laerd2020}.
    \item \textbf{Ordinal} are nominal variables that have two or more categories and are ordered or ranked \parencite{Laerd2020}.
\end{itemize}

The datasets to be examined in this thesis include continuous variables of the type interval and all three types of categorical variables (see \cref{dataset_baysis,dataset_arbis}).

\section{Correlation Types}
\label{correlation_coefficient_types}
The datasets from \acrshort{baysis}, \acrshort{arbis} (see \cref{dataset_baysis,dataset_arbis}) and the processing tool (see \cref{methodology_data_processing}) includes continuous, as well as categorical variables, describing interval, nominal, dichotomous and ordinal characteristics. For an exact analysis of relations between these characteristics the appropriate correlation coefficient suited for the respective variables needs to be chosen. This task itself is quite complex due to the number of coefficients to evaluate, amount of literature and numerous assumptions in the field of statistics. From a comprehensive literature review the following correlation coefficients and tests were selected because of their appearance in current studies and papers and in dependency of their suitability for the respective relation.

For a fundamental introduction into correlation statistic, the article of Jun Ye \textit{Everything you need to know about correlation}\footnote{https://junye0798.com/post/everythin-you-need-to-know-about-correlation/} is highly recommended \parencite{Yun2020}.

\subsection{Correlation coefficient for continuous - continuous relations}
\label{correlation_pearson}

As stated in \cref{correlation_variable_types} continuous variables are metric measurements in form of distances or durations. The most common correlation coefficient for continuous variables is the so called non-parametric Pearson's $r$.

\subsubsection{Pearson's $r$}
Pearson's $r$ describes the linear correlation of continuous, non-ranked variables and does not assume normality or a normal distributed sample set as it is non-parametric. It is therefore suitable for the examination of continuous - continuous variable relations, where finite size of variance and covariance can be assumed. \parencite{BenestyChenHuang2009,Sulthan2018}
 
The general correlation coefficient, shown in \cref{formula_correlation_basic} is the foundation for deducing Pearson's $r$. It is defined by the fraction of the covariance $\sigma_{xy}$ of two vectors $x$,$y$ of length $i$, see \cref{formula_correlation_covariant} and their standard deviation $\sigma_{x,y}$, see \cref{formula_correlation_deviation} \parencite{HerzSchlicherSiegener1992}.

\smallskip

\begin{equation}
	\label{formula_correlation_basic}
	\rho = \frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}
\end{equation}
\begin{equation}
	\label{formula_correlation_covariant}
	\sigma_{xy} = \sqrt{\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{n}}
\end{equation}
\begin{equation}
	\label{formula_correlation_deviation}
	\begin{split}
		\sigma_{x} = \sqrt{\frac{\sum_{i}^{n}(x_i-\bar{x})^2}{n}} \\ 
		\sigma_{y} = \sqrt{\frac{\sum_{i}^{n}(y_i-\bar{y})^2}{n}}
	\end{split}
\end{equation}

\bigskip

The following \cref{formula_pearson} shows Pearson's correlation coefficient $r$, which is a direct usage of the definition in \cref{formula_correlation_basic}, assuming that both data variables have the same length, named $i$. The symbols $\bar{x}$ and $\bar{y}$ correspond to the means of the data variable $x$ and $y$, respectively. \parencite{BenestyChenHuang2009,Zychlinski2018}

\smallskip

\begin{equation}
\label{formula_pearson}	
	r_{xy} =  \frac{\sum_{i}{(x_i-\bar{x})(y_i-\bar{y})}}{\sqrt{\sum_{i}{(x_i-\bar{x})^2}\sum_{i}{(y_i-\bar{y})^2}}}
\end{equation}

\bigskip

This equation can be simplified to \cref{formula_pearson_simplified} with the $SS$ corresponding to the summed squares and $SP$ corresponding to summed products.

\smallskip

\begin{equation}
\label{formula_pearson_simplified}
	r =  \frac{SP_{xy}}{\sqrt{SS_x SS_y}}
\end{equation}

\subsubsection{Interpretation of $r$}
Pearson's $r$ can have values of the range $-1$ to $+1$. If one variable moves in the same direction as the other, it is called positive correlation, represented by a positive correlation coefficient. In the case of one variable changing in a positive direction, whereas a second variable is changing in a negative direction, the correlation is called negative and has a negative coefficient. Another characteristic is the rate of change in the variables. When both variables change at the same rate, they are linearly correlated. When both variables do not change in the same rate, then they are non-linearly or curvy-linear correlated and $r$ approximate to zero, showing no correlation.

The effect size of Person's $r$ defined the absolute value of $r$, mathematic written as $|r|$. According to Cohen recommendations for mean-based coefficients (explained in \cref{correlation_effect_size}) and with consideration of the guidelines by Wolfe \parencite{Wolfe2017} and Regber \parencite{Regber2016}, which advocate an increased scale due to the high sensitivity of Pearson's $r$, the following rules are defined for the interpretation of the effect size of $r$.

\begin{itemize}
  \item When both variables change in the same ratio, the absolute value is 1.0, which is called perfect correlation.
  \item If the range is above .80, it is called high degree of correlation.
  \item A moderate degree of correlation lays in the range of .50 to .80.
  \item When the range is between .30 to .50, it is called low degree of correlation.
  \item If the range is lower than .30 no correlation can be proven. It is called absence of correlation.
\end{itemize}

\subsubsection{Significance of $r$}
To determine if $r$ is statically significant a chi-square test is generally applied to find the $p$-value, testing the probability of independence (see \cref{correlation_significance}). 

\subsection{Correlation coefficient for continuous - nominal relations}
This type of relation is objectively the most complex to evaluate. One well known method to analyze the relation between a continuous and categorical variables, which is not ranked and has more than two values, is the analysis of variance (ANOVA). Unfortunately, it assumes normal or gaussian distributed variables, which are not given in the dataset (see \cref{data}), as it is categorized as a parametric test. A non parametric approach of the ANOVA is the rather uncommon Kruskal-Wallis $h$-test \parencite{Leon1998}. Both tests indicate if at least one variable stochastically dominates another, but not in which groups or in how many groups this domination occurs \parencite{OTSD2020}. They therefore do not provide a statement about the correlation strength, but the statistical significance of variances between groups.

Further research of a non-parametric correlation coefficient only provided one suitable option, the eta ($\eta$) coefficient by Pearson \parencite{Benninghaus2007}, describing the relationship between variables based on the sums of squares used in the ANOVA \parencite{Lewis2012,Benninghaus2007}.

\subsubsection{Eta ($\eta$) coefficient}
% https://stackoverflow.com/questions/52083501/how-to-compute-correlation-ratio-or-eta-in-python/52084418
% https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full
The $\eta$ coefficient, also called correlation ratio, is a measurement for the proportion of the variation in $y$, which is associated with a membership of different groups in $x$ \parencite{Laken2013}. Or mathematically $\eta$ is the squared root of the ratio between $SS_x$ and $SS_y$ \parencite{Shaldehi2013,SAGE2014}. When calculated the value of $\eta^2$ represents the percentage of total variance which can be accounted to a group relation \parencite{Laken2013}.

\smallskip
\begin{equation}
\label{formula_eta}	
	\eta = \sqrt{\frac{SS_x}{SS_y}}
\end{equation}

\subsubsection{Interpretation of $\eta$}
The coefficient $\eta$ can have values in the range from $0$ to $1$ and can be interpreted similar to Person's $r$ according to \parencite{Laken2013}. The effect size, which is categorized in the $R$ group and can therefore be interpreted as follows \parencite{Regber2016,Cohen1988}:
% https://stats.stackexchange.com/questions/166696/how-do-i-convert-eta2-to-pearsons-r
% https://www.researchgate.net/post/Whats_the_reason_for_deriving_Cohens_w_from_Cramers_V_if_the_latter_is_already_a_measure_of_effect_size
\begin{itemize}
	\item $\eta <= .06$ : the correlation strength is weak
	\item $.06 < \eta < .14$ : a moderate strength of correlation
	\item $\eta >= .14$ : there is a strong correlation.
\end{itemize}

\subsubsection{Significance of $\eta$}
The significance evaluation for the $\eta$ coefficient is generally performed via the $p$-value of the Kruskal-Wallis $h$-test, since it is a non-parametric test. It is calculated with the chi-squared test ($\chi^2$), defined in \cref{correlation_significance} \parencite{Filipiak2013}. In the case of both a correlated $\eta$ and a significant $h$-test, the exact related groups need to be determined via a post-hoc test (see \cref{correlation_posthoc}).

\subsection{Correlation coefficient for continuous - dichotomous relations}
The Point Biserial correlation is a special form of the Pearson's $r$ correlation coefficient and suited to evaluate the association of continuous-dichotomous relations. 

\subsubsection{Point Biserial}
The Point Biserial notation, shown in \cref{formula_point_biserial}, can be derived from Person's $r$ with the assumption of $y$ only taking dichotomy values of 0 and 1, so that $\bar{y} = p$. The distinction of the cases

\begin{itemize}
	\item $n \cdot p$ referring to $y=1$ an with $1 - p = q$ bigger than $\bar{y}$
	\item $n \cdot q$ referring to $y=0$ an with $0 - p = -p$ smaller than $\bar{y}$
\end{itemize}
allow to form \cref{formula_point_biserial_from_pearson} from \cref{formula_pearson}, which can be simplified to \cref{formula_point_biserial} with the intermediate step of \cref{formula_point_biserial_from_pearson_simplyfied} \parencite{Tate1954,CohenWest2003,Bortz2004,DeJesus2019}.

\smallskip

\begin{equation}
	\label{formula_point_biserial_from_pearson}
	r_{pq} =  \frac{n \cdot p (\bar{x}_{y=1}-\bar{x}) \cdot q + n \cdot p (\bar{x}_{y=0}-\bar{x}) \cdot (-q)}{\sqrt{\sum_{i}{(x_i-\bar{x})^2} \cdot (n \cdot p \cdot q^2 + n \cdot q \cdot (-p)^2)}}
\end{equation}
\begin{equation}
	\label{formula_point_biserial_from_pearson_simplyfied}
	r_{pqi} =  \frac{n \cdot p \cdot q \cdot (\bar{x}_{y=1}-\bar{x}_{y=0})}{\sqrt{\sum_{i}{(x_i-\bar{x})^2} \cdot (n \cdot p \cdot q)}}
\end{equation}
\begin{equation}
	\label{formula_point_biserial}
	r_{pq} =  \frac{\bar{x}_{y=1}-\bar{x}_{y=0}}{\sqrt{\sum_{i}{(x_i-\bar{x})^2}}} \cdot \sqrt{n \cdot p \cdot q \cdot} 
\end{equation}

\smallskip

It must be pointed out that if the dichotomous variable is artificially binarized, i.e. there is likely continuous data underlying it, biserial correlation is more a measurement of similarity instead of association \parencite{OTSD2020}.

\subsubsection{Interpretation of $r_{pq}$}
Due to the mathematical similarity of the Point Biserial to the Pearson's $r$, the general interpretation of Pearson's $r$ defined in \cref{correlation_pearson} can be applied to Point Biserial with some adjustments. The range of the Point Biserial coefficient, from $0$ to $1$ removes the direction of correlation from the interpretation. According to Cohen \parencite{Cohen1988} the following can be used as guidelines for the effect size $r_{pq}$ \parencite{Leblanc2017}:

\begin{itemize}
	\item $r_{pq} < \: .30$ : The correlation strength is weak
	\item $\: .30 =< r_{pq} < \: .50$ : a moderate strength of correlation
	\item $r_{pq} >= \: .50$ : there is a strong correlation.
\end{itemize}

\subsubsection{Significance of $r_{pq}$}
The significance evaluation of the Point Biserial coefficient is done via the 2-tailed $p$-value, a doubled chi-square test (see \cref{correlation_significance}).

\subsection{Correlation coefficient for continuous - ordinal relations}
For ordinal variables, also called ranked or rank ordered, the commonly used Spearman's $\rho$ can be applied, but should be replaced by Kendall's $\tau$ because of it superiority over Spearman \parencite{Newson2002}. 

\subsubsection{Kendall's $\tau$}
Kendalls $\tau$ evaluates the order of rank pairs, instead of the squared rank difference, which makes it more robust against outliers. Because it can be assumed that the data has ties implemented, the $\tau$ with ties must be used. The general definition is shown in \cref{formula_kendalls_r_complex}, with $P$ referring to the proversion and $I$ to the inversion. With the assumption that the continuous measurement $x$ is or can be ordered, the ordinal/ranked variable $y$ will be wrongly ordered. After forming all possible rank pairs between $x$ and $y$, $P$ and $I$ can be deduced and $\tau$ can be calculated. \parencite{Reiter2015,Bossart2017}

\begin{itemize}	
	\item[] \textbf{Proversion} ($+$) is the number if pairs, where $x < y$ 
	\item[] \textbf{Inversion} ($-$) is the number of pairs, where $x > y$ 
	\item[] \textbf{Ties} ($0$) are pairs where $x = y$
\end{itemize}

\begin{equation}
	\label{formula_kendalls_r_complex}
	\tau = \frac{P-I}{\sqrt{\left(\frac{N(N-1)}{2}-T_x\right) \cdot \left(\frac{N(N-1)}{2}-T_y\right)}}
\end{equation}
\begin{equation}
	\label{formula_kendalls_r_tx}
	T_x = \sum_{i=1}^n \frac{t_{x_i}(t_{x_i}-1)}{2}
\end{equation}
\begin{equation}
	\label{formula_kendalls_r_ty}
	T_y = \sum_{j=1}^m \frac{t_{y_j}(t_{y_j}-1)}{2}
\end{equation}
\smallskip

\begin{itemize}
	\setlength\itemsep{0.1em}	
	\item[] $N$ is the total number of rank pairs 
	\item[] $P | I$ are the pro- and inversion of pairs
	\item[] $T_x | T_y$ are the ties in $x$ and $y$
	\item[] $n | m$ are the number of rank bindings in $x$ and $y$
	\item[] $t_{x_i} | t_{x_i}$ are the length of rank bindings in $x$ and $y$
\end{itemize}

\bigskip

Through transformation of \cref{formula_kendalls_r_complex} we can simplify the general definition to equation \ref{formula_kendalls_r} \parencite{Reiter2015}.

\begin{equation}
\label{formula_kendalls_r}
	\tau = \frac{P-I}{\sqrt{(P+I+T_x) \cdot (P+I+T_y)}}
\end{equation}

\subsubsection{Interpretation of $\tau$}
% https://www.reddit.com/r/AskStatistics/comments/44ypc4/kendall_taus_effect_size_question/
Strictly speaking, Kendall's $\tau$ is not a measure of effect size, like Pearson's $r$, but tends to be of similar magnitude. Because of this similarity the general interpretation defined in \cref{correlation_pearson} and \cref{correlation_effect_size} can be applied. To adapt the guidelines to the lesser sensitivity of $\tau$, they are scaled downwards \parencite{Regber2016}.

\begin{itemize}
	\item $\tau < \: .30$ : The correlation strength is weak
	\item $\: .30 =< \tau < \: .50$ : A moderate strength of correlation
	\item $\tau >= \: .50$ : There is a strong correlation
\end{itemize}

\subsubsection{Significance}
The significance evaluation of Kendall's $\tau$ coefficient is carried out via the 2-tailed $p$-value, elaborated in \cref{correlation_significance}.

\subsection{Correlation coefficient for categorical - categorical relations}
The Pearson's $\chi^2$ test from \cref{correlation_pearson} can also be applied to categorical data for independence statistics. Two correlation coefficients using $\chi^2$ are Cramer’s $V$ and Theil’s $U$. Both can be used to analyze categorical - categorical relations, but differ in the type of result they provide \parencite{OutsideTwoStandardDeviations2018}. Cramer’s $V$ is a symmetric measure providing a measure of association strength. Theil’s $U$, the uncertainty coefficient, on the other hand is a conditional measure and represents the predictability of an association \parencite{Akoglu2018,StackExchange2020}. Therefore the Theil’s $U$ measurement it is the preferred choice for the analysis of predictability and Cramer’s $V$ is used in the correlation analysis.

\subsubsection{Cramer’s V}

Cramer’s $V$, also called Cramer's phi ($\Phi_c$), is a measurement for the relation of two nominal variables. In \cref{formula_cramers_v_biased}, showing the notation of Cramer’s $V$, $k$ and $r$ are the number of columns and rows, respectively. $\varphi$, the phi coefficient, is defined by $\frac{{\chi^2}}{n_{ij}}$. The $\chi^2$ shown in \cref{formula_cramers_v_chi} is derived from \cref{formula_chi_squared_simplified} with the expansion to columns and rows \parencite{Sheskin1997,Bergsma2013}.
\smallskip
\begin{equation}
\label{formula_cramers_v_biased}
	V = \Phi_c =  \sqrt{\frac{{\varphi^2}}{min(k-1,r-1)}} = \sqrt{\frac{\frac{{\chi^2}}{n_{ij}}}{min(k-1,r-1)}}
\end{equation}
\begin{equation}
\label{formula_cramers_v_chi}
	\chi^2 =  \sum_{i,j}{\frac{\left(n_{ij}-\frac{n_i n_j}{n}\right)^2}{\frac{n_i n_j}{n}}}
\end{equation}

\smallskip

The above notation of $\Phi_c$ can be heavily biased, trending to overestimate the strength of relation. It can be corrected with \cref{formula_cramers_v_corrected}, using the corrected notation \cref{formula_cramers_v_phi_corrected} for $\tilde{\varphi^2}$ and \cref{formula_cramers_v_k_corrected} as well as \cref{formula_cramers_v_k_corrected} for $k,r$. \parencite{Bergsma2013}
\smallskip
\begin{equation}
\label{formula_cramers_v_corrected}
	\tilde{V} = \tilde{\Phi_c} = \sqrt{\frac{\tilde{\varphi^2}}{\text{min}(\tilde{i_{\text{max}}}-1,\tilde{j_{\text{max}}}-1)}}
\end{equation}
\begin{equation}
\label{formula_cramers_v_phi_corrected}
	\tilde{\varphi^2} = \text{max}\left(0,\varphi^2 - \frac{(k-1)(r-1)}{n-1}\right)
\end{equation}
\begin{equation}
\label{formula_cramers_v_k_corrected}
	\tilde{k} = k - \frac{(k-1)^2}{n-1}
\end{equation}
\begin{equation}
\label{formula_cramers_v_r_corrected}
	\tilde{r} = r - \frac{(r-1)^2}{n-1}
\end{equation}

\bigskip

\subsubsection{Theil’s U}
% https://rstudio-pubs-static.s3.amazonaws.com/558925_38b86f0530c9480fad4d029a4e4aea68.html
The uncertainty coefficient, also called entropy coefficient, is a measurement for the association between two nominal variables and in comparison to Cramers $V$, provides a much better predictability statement. It is based on the concept of comparing the entropies of variables to determine a degree of association \parencite{Hoang2019}. The entropy of a distribution (see \cref{formula_theils_hx}) and the conditional entropy (see \cref{formula_theils_hxy}) are used to calculate the uncertainty coefficient $U(X)$ \parencite{Glen2017,Glen2018}, which tells us: given Y, what fraction can be predicted for X \parencite{Hoang2019}.

\smallskip
\begin{equation}
\label{formula_theils}
	U(X) = \frac{H(X)-H(X|Y)}{H(X)}
\end{equation}
\begin{equation}
\label{formula_theils_hx}
	H(X) = -\sum_{x} p_{X} \log_{p_X} (x)
\end{equation}
\begin{equation}
\label{formula_theils_hxy}
	H(X|Y) = -\sum_{x,y} p_{X,Y} \log_{p_{X,Y}} (x,y)
\end{equation}

% Definition: Theil’s U symmetrical
%\begin{equation}
%\label{formula_theils_sym}
%	U(X,Y) = \frac{H(X)U(X|Y)-H(Y)U(Y|X)}{H(X)+H()Y} = 2[\frac{H(X)+H(Y)-H(X|Y)}{H(X)+H(Y)}]
%\end{equation}

\subsubsection{Interpretation}
For the interpretation of Cramers $V$ an adaption to Pearson's $r$ is necessary, but at the same time quite controversial. Some studies convert $V$ to the effect size $w$ for an equal measurement to $r$ ($r$ is representable over different studies), whereas $V$ is already a measure of effect size by itself \parencite{Baguley2016}.
% https://www.researchgate.net/post/Whats_the_reason_for_deriving_Cohens_w_from_Cramers_V_if_the_latter_is_already_a_measure_of_effect_size
% https://www.researchgate.net/post/How_can_I_intepret_the_effect_sizes_of_Cramers_V_when_DF_3

\smallskip
\begin{equation}
\label{formula_cohens_w}
	w = V \cdot \sqrt{\text{min}(i_{\text{max}}-1,j_{\text{max}}-1)}
\end{equation}

\medskip

As shown in \cref{formula_cohens_w} \parencite{Baguley2016} the conversion from $V$ to $w$ is similar to the reduction of $\Phi_c$ in $V$ (see \cref{formula_cramers_v_corrected}). This conversion to $w$ is necessary within a single study and an adaption of scale is sufficient \parencite{Baguley2016}. In terms of this thesis an adapted scale of effect size according to Ellis is used to interpret $w$ in the range of $0$ to $1$ \parencite{Cohen1988,Ellis2010,Hemmerich2019}

\begin{itemize}
	\item $w < \: .30$ : the correlation strength is weak
	\item $\: .30 =< w < \: .40$ : a moderate strength of correlation
	\item $w >= \: .40$ : there is a strong correlation.
\end{itemize}

The values of Theil's $U$ show the predictability of the association, where the following rules for interpretation apply \parencite{TheilsInt01,TheilsInt02,TheilsInt03}

\begin{itemize}
	\item $U < \: 1$ : the forecasting is better than guessing
	\item $U \sim \: 1$ : the forecasting is about as good as guessing
	\item $U > \: 1$ : the forecasting is worse than guessing.
\end{itemize}

\section{Correlation Matrix}
As a result the following correlation coefficients and statistical tests are used for the mixed analysis of continuous and categorical variables. These will be implemented into a correlation processing script, which is explained in \cref{methodology_correlation_processing}.

\bigskip

\begin{table}[ht]
	\centering
    \begin{tabular}{c|c|c|c|c}
        \toprule
								& \textbf{Ordinal} 	& \textbf{Dichotomous} 	&  \textbf{Nominal}	& \textbf{Continuous}	\\
		\midrule
		\textbf{Ordinal}		& Cramer’s $V$		& Cramer’s $V$			& Cramer’s $V$		& Kendall's $\tau$		\\
		\midrule
		\textbf{Dichotomous}	& Cramer’s $V$		& Cramer’s $V$			& Cramer’s $V$		& Point Biserial $r$	\\
		\midrule
		\textbf{Nominal}		& Cramer’s $V$		& Cramer’s $V$			& Cramer’s $V$		& Eta $\eta$			\\
		\midrule
		\textbf{Continuous}		& Kendall's $\tau$ 	& Point Biserial $r$	& Eta $\eta$		& Pearson's $r$			\\
		\bottomrule
	\end{tabular}
	\caption{Selected Correlation Coefficients}
	\label{tab:correlation_coefficient_matrix}
\end{table}

\section{Effect size}
\label{correlation_effect_size}
The degree of correlation, also described as the strength of association is called effect size and shows how strong two variables are related with each other. According to Cohen \parencite{Cohen1988} the effect size of each correlation coefficient falls into one of two categories $D$ and $R$. $D$ corresponds to coefficients utilizing the mean difference and standardized mean difference. He defined the values of $D$ coefficients as small $D$ = .20, medium $D$ = .50, and large $D$ = .80 \parencite{Piegorsch2002}. The group of $R$ coefficients includes measures based on variance \parencite{Walker2005}. Cohen proposes vastly different values of .01, .06, and .14 as indicators of small, medium, and large effect sizes for the $R$ group \parencite{Cohen1988}. However, if these values fit the purpose of the analysis, depends on the underlying data and is at discretion of the researcher. This means the values for interpreting the effect size can and should be adapted to fit the underlying data and the usage of the interpretation.

% http://www.leeds.ac.uk/educol/documents/00002182.htm
% https://www.psychometrica.de/effect_size.html
% https://www.cedu.niu.edu/~walker/personal/Walker%20Kendall's%20Tau.pdf

There are many effect sizes like Cohen's $f^2$ or Heghes $g$, which can be used for the interpretation of relations. The interpretation by Cohen for $r$ is the most robust and common ground found in literature and will be used as a mathematical base for other correlation coefficients.

\section{Significance vs. Uncertainty}
\label{correlation_significance_uncertainty}
Pure correlation results are not sufficient to prove a relation between variables, especially when using the correlation evidence for predictive purposes. Depending on the sample data, statistical results can be random or biased and therefore need to be examined for their probability of error. This is commonly evaluated via the well known statistical significance. Even though statistical significance is the common standard procedure for evaluating the probability of error, it became a subject of debate in recent years if statistical significance is to be generally used and if it is potentially misleading. It is often advocated to additionally used statistical uncertainty instead of only relying on statistical significance \parencite{Harris2019}.

\subsection{Uncertainty}
\label{correlation_uncertainty}
The uncertainty assessment aims to determine if a variable has suitable statistical characteristics for the intended analysis or in colloquial terms, is \textit{fit for the purpose}. The literature research showed that there are a number of different procedures for the evaluation of uncertainty. \textit{The Evaluation of Measurement Data -- Guide to the Expression of Uncertainty in Measurement}\footnote{https://www.bipm.org/utils/common/documents/jcgm/JCGM\_100\_2008\_E.pdf} (also referred to as GUM) defines reliable but also complex guidelines and is recommended for an in depth understanding of uncertainty evaluation \parencite{Farrance2012}. The complexity of the GUM procedures (which are beyond of the scope of this thesis) and the data foundation favored the usage of simpler methods like the \textit{population sampling error} \parencite{ONS2020}.

The sampling error is the uncertainty created by selecting a subset from a population. This can result in poorly distributed variables, which induces a high probability of random correlations based on a small number of samples. An initial review of the variable distributions in the \acrshort{baysis} and \acrshort{arbis} dataset showed that there are many sample sizes below 10 (see \cref{dataset_baysis,dataset_arbis}), which is an arguably low count. Research into the minimal sample size showed, that for categorical variables, with number of categories $k$ a minimal number of 20 ($k=3$) or 200 ($k=10$) should be asserted \parencite{Cicchetti1981}. From these guidelines and the exploratory approach of this thesis, which allows for lower thresholds, a sample size below 10 are considered to be uncertain and associated correlations should be neglected by default.

\subsection{Significance}
\label{correlation_significance}
Because statistical results are usually based on sample sets, it is necessary to test if the results can be applied on the population or are significant. Therefore a decision on one of the two hypothesis of \textit{"The averages are equal to the population"} which means the statistical results can be applied on the general population and \textit{"The averages are not equal to the population"} which means the statistical results can not be applied on the general population, need to be determined. This can be evaluated by the common dependent $t$-test which compares the significance value $p$ to the error probability level $\alpha$. The $p$-value is generally calculated via chi-squared, shown in \cref{formula_chi_squared_simplified} (chi-squared statistic), which was taken from Wikipedia and cited from Karl Pearson \parencite{Pearson1990}.

% \begin{equation}
% \label{formula_chi_squared}	
% 	\chi^2 = \sum_{i=1}^{m}{\frac{(N_i-n_{0i})^2}{n_{0i}}}
% \end{equation}

\begin{equation}
\label{formula_chi_squared_simplified}	
	\chi^2 = \sum_{i=1}^{n}{\frac{(O_i-E_i)^2}{E_i}} = N\sum_{i=1}^{n}{\frac{(O_i/N-p_i)^2}{p_i}}
\end{equation}
\begin{itemize}
	\setlength\itemsep{0.1em}	
	\item[] $N$ is the total number of data samples 
	\item[] $O_i$ is the number of data samples with type $i$
	\item[] $E_i = N p_i$ is the expected number of data samples with type $i$
\end{itemize}

\medskip

The $p$-value can then be comprised by comparing $\chi^2$ to a $\chi^2$-distribution by calculation or by using a conversion table \parencite{Piegorsch2002} with the degree of freedom $df = (n_x - 1) \cdot (n_y - 1)$. The resulting  $p$-value is compared to the $\alpha$-level to either accept ($p > \alpha$) or reject ($p \le \alpha$) the null hypothesis \textit{"The means are equal to the population"}. For a 2-tailed $p$-value, $p$ will be doubled to incorporate both ends of the distribution. Usually a value of $.05$ is chosen for $\alpha$ which there will be a 5\,\% risk of falsely rejecting the null hypothesis. Out of this definition the following two interpretations of the correlation coefficient can be drawn \parencite{Tenny2020,OTSD2020}.

\begin{itemize}
	\item $p <= \alpha$ means that the null hypothesis can be rejected and indicates that there is a significant dependency between the two tested variables. It can be concluded that the increase or decrease of one variable does significantly relate to the increase or decrease of the other.
	\item $p > \alpha$ means that there is \textbf{no} significant dependency between the two variables and no conclusion can be drawn from the correlation.
\end{itemize}

\section{Post Hoc}
\label{correlation_posthoc}
In the case of a relation of a categorical variable the correlation effect size and its significance do not provide any statement on which categories are related. This needs to be further evaluated with a Post Hoc test. In our case the Post Hoc test is based on two separated tests. First the variables will be tested for significance with a Kruskal-Wallis rank sum test. 

% https://www.sciencedirect.com/topics/nursing-and-health-professions/kruskal-wallis-test
\smallskip
\begin{equation}
\label{formula_kruskal_wallis}	
	h = \frac{12}{N(N+1)}\sum_{h}{\frac{S_h^2}{n_h}}-3(N+1)
\end{equation}
\begin{itemize}
	\setlength\itemsep{0.1em}	
	\item[] $N$ is the total number of samples 
	\item[] $n$ is the number of samples in group $h$
	\item[] $S_h$ rank sum of group $h$
\end{itemize}

\medskip

For Kruskal-Wallis $h$-test the following rules apply. The higher the value of $h$, the higher is the variance between the unique variables. A high $h$ normally supports an already found correlation, but must be tested for significance with $\chi^2$ defined in \cref{correlation_significance}. If the significance or $p$-value is below the $\alpha$-level of $.05$ the null hypothesis can be rejected. This implies that the correlation or variances between the groups are significant. For the identification of which groups create this significant variance the following Wilcoxon-Mann-Whitney, sometimes also called Wilcoxon $T$-test is applied pairwise.

The Wilcoxon $T$-test is a non-parametric univariate alternative to the dependent $t$-test and is the recommended test to use when the data violates the assumption of a normal distribution. The two variations \textit{Mann-Whitney U statistic} and \text{Wilcoxon W sanked sum statistic} can be considered as equivalent. The Wilcoxon ranked sum test will be used in a pairwise approach to compare all groups with each other. 
\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
		\matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
		{
		p(G_1,G_1) & p(G_n,G_1) \\
		p(G_1,G_n) & p(G_n,G_n) \\};
		\path[-stealth]
		(m-1-1) edge node [left] {$1,...,n$} (m-2-1)
				edge (m-1-2)
				edge [dashed,-] node [below] {$p$} node [above] {$0$} (m-2-2)
		(m-2-1.east|-m-2-2) edge node [below] {$1,...,n$} (m-2-2)
		(m-1-2) edge node [right] {} (m-2-2);
  	\end{tikzpicture}	
  	\caption{2-dimensional Wilcoxon ranked sum test significance matrix}
\end{figure}

From the resulting matrix of $p$-values for each group ($G_{1,...n}$) and the analog interpretation with the $\alpha$-level (see \cref{correlation_significance}) the significant groups can be identified. These identified groups are considered to have a significant difference to the other groups and therefore can predictably point to a measurement characteristic from a category or vice versa.

A non significant difference does not mean that there is no correlation, but only that the sample set and size can not support significance. There are also cases where the $p$-value of the correlation coefficient and the general Kruskal-Wallis rank sum test show significant differences in the variable, but the pairwise Wilcoxon ranked sum test does not show any significant differences between the groups. This unfitting difference between the \textit{global} and \textit{specific} significance appears when variables have differences between the samples, but the differences are not reflected by the groups or the sample size is not sufficient to support the significance of the differences. These controversial correlations can also be evaluated in a exploratory data analysis, keeping in mind their limited interpretability and predictability.

To define what the significant, as well as the non significant groups are referring to in the variable, the statical indicators of count ($n$), mean ($\bar{x}$), standard deviation ($\sigma$), median ($\tilde{x}$), $min$, $max$ and range ($\Delta$) are considered. These are quite common for describing statical data and can be considered as robust. These indicators can be presented as a table or visual plot, as shown in \cref{tbl:descriptives_sample,fig:descriptives_sample}.
\pgfplotstableread[col sep=comma,header=false]{
	G1 , 0 , 70  , 60  , 50 
	G2 , 0 , 90  , 95  , 70 
	G3 , 0 , 100 , 110 , 80  
	G4 , 0 , 110 , 115 , 90  
	G5 , 0 , 130 , 150 , 110 
}\data
\pgfplotstablecreatecol[
  create col/expr={\thisrow{1} + \thisrow{2} + \thisrow{3} + \thisrow{4}}
]{sum}{\data}
\begin{figure}[ht]
	\centering
	\begin{minipage}{0.5\textwidth}
		\tiny
		\setlength{\tabcolsep}{4pt}
		\centering
		\begin{tabular}{c|c|c|c|c|c|c|c}
			\toprule
			Group & $n$ & $\bar{x}$ & $\sigma$ & $\tilde{x}$ & $min$ & $max$ & $\Delta$ \\
			\midrule
			$G_1$     & $n_1$  	  & $\bar{x}_1$ 	  & $\sigma_1$ 	   & $\tilde{x}_1$ 	   & $min_1$ 	 & $max_1$     & $\Delta_1$	 	\\ 
			...   	  & ...    	  & ... 	   	 	  & ...	  	  	   & ...		       & ...   		 & ... 	  	   & ... 	   	    \\ 
			$G_{n-1}$ & $n_{n-1}$ & $\bar{x}_{n-1}$   & $\sigma_{n-1}$ & $\tilde{x}_{n-1}$ & $min_{n-1}$ & $max_{n-1}$ & $\Delta_{n-1}$ \\ 
			$G_n$     & $n_n$  	  & $\bar{x}_n$ 	  & $\sigma_n$ 	   & $\tilde{x}_n$ 	   & $min_n$ 	 & $max_n$ 	   & $\Delta_n$ 	\\ 
			\bottomrule
		\end{tabular}
		\subcaption[second caption.]{Table of all descriptives}\label{tbl:descriptives_sample}
	\end{minipage}%
	\begin{minipage}{0.55\textwidth}
		\tiny
		\centering
		% https://texwelt.de/fragen/26286/zwei-gruppen-bei-einem-gestapelten-balkendiagramm-und-zentrierte-balkenbeschriftungen
		% https://tex.stackexchange.com/questions/275229/forcing-all-axis-labels-to-display-in-a-plot
		\begin{tikzpicture}
			\begin{axis}[
				% Gitterlinien für y-Ticks
				width=\textwidth,
				height=5.5cm,
				xmajorgrids=true,
				ymajorgrids=true,
				xtick=data,
				xmin=0,xmax=4,
				xticklabels from table={\data}{[index]0},
				% zusätzliche Beschriftung an y-Achse
				every extra y tick/.style={
					tick0/.initial=blue,
					tick1/.initial=red,
					tick2/.initial=brown,
					yticklabel style={
						color=\pgfkeysvalueof{/pgfplots/tick\ticknum}
					},
				},
				extra y ticks={100,110,80},
			]
			\addplot table [absolute series=2] {\data};
			\addplot table [absolute series=3] {\data};
			\addplot table [absolute series=4] {\data};
			\legend{
				$\bar{x}$,$\sigma$,$\tilde{x}$}
			\end{axis}
		 \end{tikzpicture}\vfill
		\subcaption[second caption.]{Plot of descriptives $\bar{x}$, $\sigma$ and $\tilde{x}$}\label{fig:descriptives_sample}
	\end{minipage}%
	\caption{Group descriptives example}
	%\vspace{-8mm}
\end{figure}

The example table shows the values of all descriptives for all groups, whereas the example figure on the right show a plot of mean ($\bar{x}$), standard deviation ($\sigma$) and median ($\tilde{x}$) with the color coded means. The figure also shows the means of the variables $\bar{x}$, $\sigma$ and $\tilde{x}$ in there corresponding color code. From these the absolute value of difference between the groups can be deviated by looking for outliners or distinct variances in the distributions, which can be interpreted accordingly. The indicators of $min$, $max$ and range ($\Delta$) can give clues what distributions should be expected and if the variable is sufficiently distributed. 

%\todo{Ergänze Effektstärke für Mann-Whitney-U}
%https://statistikguru.de/spss/mann-whitney-u-test/effektstaerke-berechnen-3.html